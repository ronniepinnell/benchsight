
Benchsight BLB ↔ NHL Player Comparison – ETL Plan
Generated: 2025-12-26T20:24:01.239936

=================================================
0. Scope and Goals
=================================================
This document describes the end-to-end ETL plan to build BLB ↔ NHL player comparisons for the Benchsight project.

The goal is to:
  • Derive per-player feature vectors for BLB (Beer League Beauties) and NHL players.
  • Normalize these into a shared feature space (z-scores per position/league).
  • Compute similarity between BLB and NHL players and store the best “comps”.
  • Expose these results in a mart layer and via dashboards.

This plan is designed to fit into your existing multi-stage architecture:
  • RAW   → STAGE → INTERMEDIATE → MART
  • Plus a comparison layer on top of MART.

This document is *logic-only*; you or an LLM can generate specific SQL / Python code
from these steps using the table & stats catalogs already provided.

=================================================
1. High-Level Data Flow
=================================================

1. BLB pipeline (already exists in your project)
   • Ingest game tracking + BLB_Tables into STAGE (Postgres).
   • Transform to INTERMEDIATE:
       - fact_events_long
       - fact_events_wide
       - fact_shifts_long
       - fact_shifts_wide
       - fact_player_boxscore (basic + advanced stats)
   • Aggregate and normalize into MART tables for game / season analytics.

2. NHL pipeline (new but structurally similar)
   • Ingest NHL data (e.g., MoneyPuck CSVs, NHL API responses) into STAGE_NHL.
   • Transform to INTERMEDIATE_NHL:
       - fact_nhl_events (if available)
       - fact_nhl_shifts (if available)
       - fact_nhl_player_boxscore
   • Aggregate and normalize into MART_NHL tables with the *same* stat definitions
     as the BLB mart where possible.

3. Comparison layer
   • Build player-level feature tables:
       - fact_blb_player_features
       - fact_nhl_player_features
   • Standardize each metric into z-scores by league/position.
   • Compute similarity between each BLB player-feature row and NHL player-feature rows.
   • Store best matches in fact_blb_nhl_player_comps.
   • Expose a view vw_blb_player_comp_cards for visualizations and reports.

=================================================
2. BLB-side ETL Steps
=================================================

2.1 Inputs
----------
You already have:
  • dim_player, dim_team, dim_season, dim_game, dim_dates, etc.
  • fact_events (long & wide), fact_shifts, fact_player_boxscore, etc.
  • These live in MART (beer league) and are already used for advanced stats.

2.2 Build fact_blb_player_features
----------------------------------

Grain: one row per BLB (player_id, season_id, feature_grain).
Typical feature_grain:
  • 'season' – aggregated across all tracked games in a given season.
  • Optionally 'all_time' or 'tournament', etc.

Steps:

(1) Aggregate core boxscore stats
    FROM: fact_player_boxscore (BLB mart)
    GROUP BY player_id, season_id
    Compute:
      • goals, primary_assists, secondary_assists
      • points, shots_on_goal, shot_attempts, blocked_shots, penalties
      • corsi_for, corsi_against, fenwick_for, fenwick_against
      • xG_for, xG_against (if available)
      • on_ice_goals_for, on_ice_goals_against

(2) Compute rates and percentages
    • toi_minutes = total_toi_seconds / 60.0
    • goals_per_60 = 60 * goals / total_toi_seconds
    • shots_per_60, shot_attempts_per_60, etc.
    • CF% = corsi_for / (corsi_for + corsi_against)
    • FF% = fenwick_for / (fenwick_for + fenwick_against)
    • xGF% = xG_for / (xG_for + xG_against), when available.

(3) Usage / deployment stats
    FROM: fact_shifts_long / fact_player_toi tables
    • OZ/NZ/DZ starts and %:
        oz_start_count, dz_start_count, nz_start_count
        oz_start_pct = oz_start_count / (oz+dz+ nz)
        etc.
    • PP/PK/EV time splits:
        ev_toi, pp_toi, pk_toi, and their percentages.

(4) Microstats (if available in BLB data)
    FROM: event_long tables with event_type / play_detail fields
    • zone_entries_per_60 (controlled vs dump-in)
    • zone_exits_per_60 (controlled vs failed)
    • dangerous_passes_per_60, pass_completion%, etc.
    • turnovers_for, giveaways, takeaways, etc.

(5) Join with dim_player + dim_team + dim_season
    • Add position_group (F/D/G), position (C, LW, etc. if available).
    • Add rating (2–6) from dim_player.
    • This creates context features that can be used later as filters or features.

(6) Write to MART table fact_blb_player_features
    Suggested columns:
      - blb_player_feature_id (PK, surrogate)
      - player_id (FK → dim_player)
      - team_id (FK → dim_team)
      - season_id (FK → dim_season)
      - feature_grain (e.g., 'season')
      - games_played_tracked
      - toi_seconds_total
      - all computed basic and advanced metrics
      - meta columns: created_at, updated_at, feature_version


2.3 Standardize BLB features (z-scores)
---------------------------------------

Create a derived view or table fact_blb_player_features_std:

(1) For each metric M (e.g., goals_per_60, CF%, xGF%):
    • Compute mean μ_M and std dev σ_M within:
        (league: BLB, position_group, season)
    • This can be done via a window function or a pre-aggregated table.

(2) Compute z-score:
    z_M = (M - μ_M) / NULLIF(σ_M, 0)

(3) Store:
    • z_goals_per_60, z_cf_pct, z_xgf_pct, etc.

You can store this:
  • As a materialized table fact_blb_player_features_std, or
  • As a view where z-scores are computed on the fly (materialized is typically easier for comps).

=================================================
3. NHL-side ETL Steps
=================================================

3.1 Inputs
----------
• MoneyPuck CSVs (or similar) with per-game, per-season stats.
• NHL API outputs (optionally) for advanced stats or per-shift data.

Minimally you want per-player-per-season stats such as:
  • games played, goals, assists, points
  • shots, shot attempts, xG, etc.
  • TOI, PP TOI, PK TOI
  • Possibly on-ice shot attempts for/against for Corsi/Fenwick.

3.2 Stage NHL data
------------------
(1) Load raw CSV/API responses into a STAGE_NHL schema.
    • stage_nhl_player_game
    • stage_nhl_player_season
    • stage_nhl_teams
    • stage_nhl_players

(2) Normalize to INTERMEDIATE_NHL tables:
    • dim_nhl_player, dim_nhl_team, dim_nhl_season
    • fact_nhl_player_game
    • fact_nhl_player_season (aggregated from game-level if needed)

3.3 Build fact_nhl_player_features
----------------------------------
Grain: one row per (nhl_player_id, nhl_season_id, feature_grain='season').

(1) Aggregate core stats:
    FROM: fact_nhl_player_season
    • goals, primary_assists, points
    • shots, shot_attempts
    • xG, xGA (if available)
    • on_ice shot attempts for/against (Corsi), unblocked shot attempts (Fenwick).

(2) Compute rates and percentages (same formulas as BLB):
    • per-60, CF%, FF%, xGF%, etc.

(3) Usage/deployment stats:
    • OZ/DZ/NZ start %, if available.
    • PP/PK/EV splits.

(4) Add context:
    • Join to dim_nhl_player, dim_nhl_team, dim_nhl_season.
    • Add position_group, position, shooting side if available.

(5) Write to MART_NHL.fact_nhl_player_features with similar columns to BLB:
    - nhl_player_feature_id (PK)
    - nhl_player_id (FK)
    - nhl_team_id
    - nhl_season_id
    - feature_grain
    - computed metrics
    - meta columns

3.4 Standardize NHL features
----------------------------
Create fact_nhl_player_features_std with same z-score approach, but partitioned by:
  • league: NHL
  • position_group
  • season

Now both BLB and NHL feature tables are in the same standardized feature space.

=================================================
4. Comparison Layer ETL
=================================================

4.1 Design feature set and weights
----------------------------------
Before computing distances, define:
  • Which standardized metrics to include, e.g.:
      z_goals_per_60, z_primary_assists_per_60, z_shots_per_60,
      z_cf_pct, z_ff_pct, z_xgf_pct,
      z_entries_per_60, z_exits_per_60, z_dangerous_passes_per_60,
      z_ev_toi_share, z_pp_toi_share, z_pk_toi_share, etc.
  • Optional feature weights:
      - offense-heavy comps vs defense-first comps
      - microstat-focused vs scoring-focused profiles

This can be stored in a small config table:
  • dim_feature_weight(feature_name, weight, enabled_flag)

4.2 Compute similarity for each BLB player
------------------------------------------

High-level algorithm (Python or SQL):

FOR each BLB player feature row f_BL in fact_blb_player_features_std:
  1. Filter candidate NHL rows f_NHL to same position_group (F/D/G) and maybe season era.
  2. For each candidate:
       - Extract vector of shared features.
       - Apply weights.
       - Compute distance (e.g., weighted Euclidean):
           d^2 = Σ_j (w_j * (z_BL_j - z_NHL_j)^2)
       - Optionally convert to similarity_score = 1 / (1 + d).
  3. Sort by distance ascending (or similarity descending).
  4. Keep top N results (e.g., top 5 or top 10).
  5. Write one row per BLB–NHL pair into fact_blb_nhl_player_comps.

Implementation options:

• Pure SQL:
  - Use a CROSS JOIN between one BLB player and all eligible NHL players,
    compute distances in an expression, then ORDER BY and LIMIT.
  - For large datasets, you’d typically pre-compute in batches or use indexing/approximate search.

• Python + SQL:
  - Pull standardized BLB and NHL features into pandas / numpy.
  - Compute distances with vectorized operations.
  - Write results back to Postgres via df.to_sql or bulk insert.

4.3 fact_blb_nhl_player_comps schema
------------------------------------
Suggested columns:

  blb_nhl_comp_id       (PK, surrogate)
  blb_player_feature_id (FK → fact_blb_player_features_std)
  nhl_player_feature_id (FK → fact_nhl_player_features_std)
  comp_rank             (1 = closest)
  distance              (numeric) – Euclidean or other
  similarity_score      (numeric) – optional
  feature_profile_name  (e.g. 'all_round', 'offense', 'defense')
  created_at
  updated_at

4.4 vw_blb_player_comp_cards view
---------------------------------
Create a view to make dashboards easy:

  SELECT
    blb.player_id,
    blb_player.name        AS blb_player_name,
    blb_team.name          AS blb_team_name,
    blb_season.name        AS blb_season,
    nhl_player.name        AS nhl_player_name,
    nhl_team.name          AS nhl_team_name,
    nhl_season.name        AS nhl_season,
    c.comp_rank,
    c.distance,
    c.similarity_score,
    -- core standardized stats side-by-side
    blb.z_goals_per_60      AS blb_z_goals_per_60,
    nhl.z_goals_per_60      AS nhl_z_goals_per_60,
    blb.z_cf_pct            AS blb_z_cf_pct,
    nhl.z_cf_pct            AS nhl_z_cf_pct,
    ...
  FROM fact_blb_nhl_player_comps c
  JOIN fact_blb_player_features_std blb
    ON blb.blb_player_feature_id = c.blb_player_feature_id
  JOIN fact_nhl_player_features_std nhl
    ON nhl.nhl_player_feature_id = c.nhl_player_feature_id
  JOIN dim_player blb_player
    ON blb_player.player_id = blb.player_id
  JOIN dim_team blb_team
    ON blb_team.team_id = blb.team_id
  JOIN dim_season blb_season
    ON blb_season.season_id = blb.season_id
  JOIN dim_nhl_player nhl_player
    ON nhl_player.nhl_player_id = nhl.nhl_player_id
  JOIN dim_nhl_team nhl_team
    ON nhl_team.nhl_team_id = nhl.nhl_team_id
  JOIN dim_nhl_season nhl_season
    ON nhl_season.nhl_season_id = nhl.nhl_season_id;

=================================================
5. Where This Fits in the Orchestrator
=================================================

Assuming a single Python orchestrator main.py, you’d add an ETL stage roughly like:

  1. Run BLB mart refresh (events/shifts/boxscore).
  2. Run NHL mart refresh (if needed / scheduled).
  3. Build/refresh:
      - fact_blb_player_features
      - fact_blb_player_features_std
      - fact_nhl_player_features
      - fact_nhl_player_features_std
  4. Compute BLB–NHL comps into fact_blb_nhl_player_comps.
  5. Refresh vw_blb_player_comp_cards.

These steps can be broken into idempotent tasks with flags/env vars like:
    --run-blb-features
    --run-nhl-features
    --run-player-comps

=================================================
6. What to Ask an LLM To Do With This
=================================================

When you upload this doc into a new LLM session, you can ask:

  • “Using this ETL plan and the existing Benchsight stats + table catalogs,
     write the SQL (Postgres) to create fact_blb_player_features,
     fact_blb_player_features_std, fact_nhl_player_features, fact_nhl_player_features_std,
     and fact_blb_nhl_player_comps, plus the view vw_blb_player_comp_cards.”

  • “Generate Python code that reads BLB and NHL feature tables from Postgres,
     computes standardized feature vectors and pairwise distances, and writes
     the top-5 NHL comps per BLB player into fact_blb_nhl_player_comps.”

  • “Update the Power BI/Dash dashboards to add a ‘Player Comps’ page
     driven by vw_blb_player_comp_cards.”

=================================================
7. Short-hand phrase to request this bundle
=================================================

If you want ChatGPT to give you this ETL plan bundle again (without retyping everything),
you can say something like:

    “Send me the latest Benchsight BLB–NHL player comps ETL bundle.”

or

    “Give me the BLB–NHL comparison ETL docs zip.”

I’ll then return or regenerate the same style of bundle, including this document.
