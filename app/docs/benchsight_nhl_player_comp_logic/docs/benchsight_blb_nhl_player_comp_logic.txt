Benchsight BLB–NHL Player Comparison Logic (Human-Facing)
=========================================================

This document explains, in plain English, how to think about comparing Beer League Beauties
(BLB) players to NHL players using the Benchsight project.

It focuses on *logic only* — not specific code — so you can reason about the design, check
assumptions, and later ask an LLM (or write your own code) to implement it using the existing
Benchsight datamart.

1. Why BLB–NHL player comps?
----------------------------
Core goals:
  - Give BLB players a sense of “who they play like” in the NHL.
  - Use comparisons to:
      * communicate strengths/weaknesses in a more intuitive way,
      * suggest development priorities (“to be more like X, you need more Y”),
      * showcase your project to coaches, teams, and potential employers.

Conceptually, every player becomes a point in a multi-dimensional “skill space”.
BLB players and NHL players live in the *same* space; distance in that space means
“how similar they are.”

2. Data sources (high level)
----------------------------
- BLB data:
    * Detailed event tracking (shots, passes, zone entries/exits, turnovers, etc.).
    * Shift data (time-on-ice, lines, situations).
    * Microstats (if/when available).
    * Rating information (2–6 scale) from your dim_player tables.

- NHL data:
    * Public sources like MoneyPuck, Natural Stat Trick, NHL Edge, NHL API, etc.
    * These provide game-by-game, season-level stats such as:
        - shots, xG, assists, goals, on-ice shot share, deployment, etc.

3. Choosing a “grain”
---------------------
“Grain” means what one row in your comparison table represents. Some options:
  - Player-season: one row per player per season (e.g., BLB 2024, NHL 2023–24).
  - Player-game: one row per player per game (fine-grained, but noisy).
  - Rolling windows: e.g., last 5 games.

For a first version, a good place to start is:
  - BLB player-season (or player-all-tracked-games).
  - NHL player-season.

4. Building feature vectors
---------------------------
A **feature vector** is just a list of numeric descriptors for each player.

Think of it as: `[goals_per_60, primary_assists_per_60, entries_per_60, ...]`.

Categories of features:

  A) Production / boxscore stats
     - Goals, primary assists, total points per 60 minutes.
     - Shots per 60, shot attempts per 60.
     - Power-play and penalty-kill production rates.

  B) On-ice shot share & quality (possession & chance generation)
     - Corsi For% (CF%): (on-ice shot attempts for vs against).
     - Fenwick For% (FF%): unblocked shots.
     - Expected Goals For% (xGF%) where available.
     - Scoring chance rates (high-danger, mid, low).

  C) Usage & deployment
     - Total TOI, TOI per game.
     - Share of team TOI (%).
     - Offensive/defensive zone start percentages.
     - Power-play and penalty-kill deployment share.

  D) Microstats (BLB-specific, later for NHL if available)
     - Controlled entries per 60; entry carry-in vs dump-in.
     - Controlled exits per 60.
     - Pass attempt rate, pass success rate.
     - Retrievals, forecheck pressures, puck touches, etc.

  E) Context / role features
     - Position group (Forward / Defense / Goalie).
     - Primary position (LW, C, RW, LD, RD) if known.
     - Shooting side (L/R) if available.
     - BLB rating tier (2–6), and maybe how often they play with/against strong players.

For each feature you include:
  - Define exactly:
      * what table it comes from,
      * which rows are counted (filters),
      * what the numerator is,
      * what the denominator is (time, events, etc.),
      * whether you’re calculating a *rate* (per 60) or a *percentage*.

5. Normalizing BLB and NHL
--------------------------
Raw counts can’t be compared across different leagues or different amounts of ice time.
Two main steps:

  1) Convert to rates
     - For example, goals per 60 minutes:
       `goals_per_60 = 60 * (total_goals) / (total_time_on_ice_minutes)`
     - Percent-based metrics (like CF%) are already normalized in some sense.

  2) Standardize (z-scores or percentiles)
     - Within each universe (BLB and NHL), compute:
         * mean and standard deviation of each stat for players at the same position.
     - Transform each feature:
         `z = (value - mean) / std`
     - Now features are in “standard deviation units” and different stats can be combined.

Later you can calibrate distributions:
  - For example, if you want BLB 0-z players to match mid-tier NHL players,
    you can shift/scale BLB scores accordingly.

6. Distance / similarity metrics
--------------------------------
After you have standardized feature vectors for BLB and NHL players:

  - A BLB player has vector `f_BL`.
  - Each NHL player has `f_NHL_i`.

We measure how *far apart* these points are:

  - **Euclidean distance**:
      `distance = sqrt( Σ (f_BL[j] - f_NHL_i[j])^2 )`
  - **Cosine similarity**:
      similarity = cosine angle between vectors (focuses on shape rather than magnitude).

Smaller distance (or higher cosine similarity) → more similar players.

In practice you also:
  - Apply feature weights (maybe shooting features matter more than hits).
  - Remove or down-weight noisy features (small-sample stuff).

7. Designing the comparison tables
----------------------------------
Imagine you already have two feature tables:

  - `fact_blb_player_features`:
      * One row per BLB player-season.
      * Includes both raw and standardized stats.
      * Keyed by `blb_player_feature_id` plus `(player_id, season_id)`.

  - `fact_nhl_player_features`:
      * Same but for NHL players, keyed by `nhl_player_feature_id`.

Then build a **comps table**:

  - `fact_blb_nhl_player_comps`:
      * Each row says: BLB player X is most similar to NHL player Y with a given score.
      * Columns:
          - blb_player_feature_id
          - nhl_player_feature_id
          - similarity_score (or distance)
          - rank (1..N comps per BLB player)
          - maybe a summary JSON of “feature deltas”.

Lastly, create a **view** for dashboards:

  - `vw_blb_player_comp_cards`:
      * Joins `fact_blb_nhl_player_comps` with:
          - BLB dims: `dim_player`, `dim_team`, `dim_season`.
          - NHL dims: `dim_nhl_player`, `dim_nhl_team`, `dim_nhl_season`.
      * Includes player names, teams, seasons, positions, and key features.

8. How dashboards might use this
--------------------------------
Examples:

  - BLB Player Card:
      * Show BLB player’s radar plot vs top 3 NHL comps.
      * List “You most resemble these NHL players: …”.

  - Archetype Explorer:
      * Cluster NHL players by feature vectors.
      * For each BLB player, show “which archetype” they belong to.
      * Example archetypes:
          - “Offensive defenseman”
          - “Shutdown center”
          - “Net-front grinder”

  - Development Path view:
      * For a BLB player and chosen NHL target:
          - Show features where BLB is below target,
          - Estimate what improvement (e.g., +0.5 shots/60, +3% CF%) would make them similar.

9. How this fits the rest of the project
----------------------------------------
Practical flow:

  1) BLB side:
       - Compute player-level stats from your existing datamart tables
         (events, shifts, boxscores, chains, etc.).
       - Aggregate to player-season or player-all-games and store as `fact_blb_player_features`.

  2) NHL side:
       - Ingest NHL data (e.g., from MoneyPuck flat files).
       - Transform to the *same* feature definitions.
       - Store as `fact_nhl_player_features`.

  3) Comparison layer:
       - Standardize features and compute distances.
       - Store top N comps in `fact_blb_nhl_player_comps`.

  4) Dashboards:
       - Use views like `vw_blb_player_comp_cards` to display comparisons.

10. How to use this document with an LLM
----------------------------------------
When you give this project to an LLM:

  - First, point it to:
      * this file,
      * the LLM README,
      * the stats and tables catalogs.
  - Then ask for:
      * concrete SQL DDL and ETL steps to build the feature and comparison tables,
      * DAX or Python/Dash code to visualize comps,
      * careful documentation of each stat and design choice.

This keeps humans and LLMs aligned on *what* you’re trying to do,
so they can focus on *how* to implement it with the rest of your Benchsight project.