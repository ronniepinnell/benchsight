<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ETL GUIDE</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; }
        h2 { color: #34495e; margin-top: 30px; }
        h3 { color: #7f8c8d; }
        code {
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Consolas', monospace;
        }
        pre {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }
        pre code {
            background: none;
            color: inherit;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        th { background: #3498db; color: white; }
        tr:nth-child(even) { background: #f9f9f9; }
        a { color: #3498db; }
        hr { border: none; border-top: 1px solid #ddd; margin: 30px 0; }
        .success { color: #27ae60; }
        .warning { color: #f39c12; }
        .error { color: #e74c3c; }
    </style>
</head>
<body>
<h1>ETL Pipeline Guide</h1>
<strong>Version:</strong> 2.0  
<strong>Date:</strong> December 30, 2025
<hr>
<h2>Overview</h2>
<p>The ETL (Extract, Transform, Load) pipeline processes raw game tracking data from Excel files into the normalized star schema in Supabase.</p>
<pre><code>Raw Excel Files → ETL (Python) → CSV Files → Loader → Supabase</code></pre>
<hr>
<h2>Pipeline Architecture</h2>
<h3>Data Flow</h3>
<pre><code>data/raw/
├── BLB_Tables.xlsx              # Dimension tables
└── [game_id]/
    ├── events_tracking.xlsx     # Event data
    └── shifts_tracking.xlsx     # Shift data
          │
          ▼
      etl.py (orchestrator)
          │
          ▼
      src/etl/<em>.py (transformations)
          │
          ▼
data/output/
├── dim_</em>.csv (44 files)         # Dimension tables
├── fact_<em>.csv (51 files)        # Fact tables
└── qa_</em>.csv (1 file)            # Quality checks
          │
          ▼
scripts/load_all_tables.py
          │
          ▼
Supabase PostgreSQL (96 tables)</code></pre>
<hr>
<h2>Running ETL</h2>
<h3>Basic Usage</h3>
<pre><code><h1>Process all games in data/raw/</h1>
python etl.py
<h1>Process specific games</h1>
python etl.py --games 18969,18977,18981,18987
<h1>Dry run (no output)</h1>
python etl.py --dry-run
<h1>With validation</h1>
python etl.py --validate
<h1>Verbose output</h1>
python etl.py --verbose</code></pre>
<h3>Configuration</h3>
<p>ETL settings in <code>config/config.ini</code>:</p>
<pre><code>[etl]
games = 18969,18977,18981,18987
process_tracking = true
enhance_stats = true
validate = true</code></pre>
<hr>
<h2>Adding a New Game</h2>
<h3>Step 1: Prepare Raw Data</h3>
<pre><code><h1>Create game folder</h1>
mkdir data/raw/19001
<h1>Add tracking files</h1>
<h1>- events_tracking.xlsx (event data)</h1>
<h1>- shifts_tracking.xlsx (shift data)</h1></code></pre>
<h3>Step 2: Run ETL</h3>
<pre><code>python etl.py --games 19001</code></pre>
<h3>Step 3: Validate</h3>
<pre><code><h1>Run tests</h1>
python -m pytest tests/test_data_quality.py -v</code></pre>
<h3>Step 4: Load to Supabase</h3>
<pre><code>python scripts/load_all_tables.py --upsert</code></pre>
<hr>
<h2>ETL Modules</h2>
<h3>Core Modules</h3>
<td> Module </td> Purpose <td>
</td>--------<td>---------</td>
<td> <code>etl.py</code> </td> Orchestrator <td>
</td> <code>src/etl/extract.py</code> <td> Read raw files </td>
<td> <code>src/etl/transform.py</code> </td> Transform data <td>
</td> <code>src/etl/load.py</code> <td> Write CSVs </td>
<td> <code>src/etl/validate.py</code> </td> Data validation <td>
<h3>Transformation Modules</h3>
</td> Module <td> Purpose </td>
<td>--------</td>---------<td>
</td> <code>src/etl/events.py</code> <td> Event processing </td>
<td> <code>src/etl/shifts.py</code> </td> Shift processing <td>
</td> <code>src/etl/players.py</code> <td> Player stats </td>
<td> <code>src/etl/teams.py</code> </td> Team stats <td>
</td> <code>src/etl/enhanced.py</code> <td> Advanced stats </td>
<hr>
<h2>Schema Changes</h2>
<h3>Adding a New Column</h3>
<p>1. <strong>Update CSV generation</strong> in relevant ETL module:</p>
<pre><code><h1>In src/etl/events.py</h1>
def transform_events(df):
    # Add new column
    df['new_column'] = calculate_new_value(df)
    return df</code></pre>
<p>2. <strong>Update schema SQL</strong>:</p>
<pre><code>-- Add to sql/05_FINAL_COMPLETE_SCHEMA.sql
ALTER TABLE fact_events ADD COLUMN new_column TEXT;</code></pre>
<p>3. <strong>Or regenerate schema</strong> from CSV:</p>
<pre><code><h1>Update SQL generator to include new column</h1>
python scripts/generate_schema.py</code></pre>
<p>4. <strong>Run in Supabase</strong>:
<pre><code>-- Or drop and recreate table
DROP TABLE fact_events CASCADE;
-- Then run CREATE TABLE from schema</code></pre></p>
<p>5. <strong>Reload data</strong>:
<pre><code>python scripts/load_all_tables.py --table fact_events --upsert</code></pre></p>
<h3>Adding a New Table</h3>
<p>1. <strong>Create CSV output</strong> in ETL:</p>
<pre><code><h1>In relevant module</h1>
def generate_new_table(data):
    df = pd.DataFrame(data)
    df.to_csv('data/output/new_table.csv', index=False)</code></pre>
<p>2. <strong>Schema is auto-generated</strong> from CSV by loader</p>
<p>3. <strong>Add to schema SQL</strong> if manual control needed</p>
<h3>Renaming/Removing Columns</h3>
<p>1. <strong>Update ETL</strong> to output new column names
2. <strong>Run migration SQL</strong>:
<pre><code>ALTER TABLE fact_events RENAME COLUMN old_name TO new_name;
-- or
ALTER TABLE fact_events DROP COLUMN old_column;</code></pre>
3. <strong>Reload data</strong></p>
<hr>
<h2>Data Quality Validation</h2>
<h3>Built-in Validations</h3>
<td> Check </td> Description <td>
</td>-------<td>-------------</td>
<td> Primary key uniqueness </td> No duplicate PKs <td>
</td> Foreign key integrity <td> All FKs exist in parent </td>
<td> Required fields </td> No nulls where required <td>
</td> Data types <td> Values match expected types </td>
<td> Business rules </td> Goals match official |
<h3>Running Validations</h3>
<pre><code><h1>All validation tests</h1>
python -m pytest tests/test_data_validation.py -v
<h1>Specific checks</h1>
python -m pytest tests/test_referential_integrity.py -v
python -m pytest tests/test_business_logic.py -v</code></pre>
<h3>Adding Custom Validations</h3>
<pre><code><h1>In tests/test_data_validation.py</h1>
def test_new_validation():
    df = pd.read_csv('data/output/fact_events.csv')
    
    # Your validation logic
    assert df['new_column'].notna().all(), "new_column should not be null"</code></pre>
<hr>
<h2>Handling Data Issues</h2>
<h3>Duplicate Keys</h3>
<pre><code><h1>In ETL, ensure unique keys</h1>
df = df.drop_duplicates(subset=['event_key'], keep='first')</code></pre>
<h3>Missing Data</h3>
<pre><code><h1>Fill with defaults</h1>
df['column'] = df['column'].fillna('default_value')
<h1>Or filter out</h1>
df = df[df['column'].notna()]</code></pre>
<h3>Invalid Data</h3>
<pre><code><h1>Validate and flag</h1>
df['is_valid'] = df.apply(validate_row, axis=1)
invalid = df[~df['is_valid']]
if len(invalid) > 0:
    invalid.to_csv('data/output/qa_invalid_rows.csv')</code></pre>
<hr>
<h2>Performance Optimization</h2>
<h3>For Large Datasets</h3>
<pre><code><h1>Process in chunks</h1>
chunk_size = 10000
for chunk in pd.read_csv(file, chunksize=chunk_size):
    process_chunk(chunk)
<h1>Use efficient types</h1>
df['int_col'] = df['int_col'].astype('Int64')  # Nullable int
df['str_col'] = df['str_col'].astype('string')  # String dtype</code></pre>
<h3>Memory Management</h3>
<pre><code><h1>Release memory</h1>
del large_df
import gc
gc.collect()</code></pre>
<hr>
<h2>Future Enhancements</h2>
<h3>ML/CV Integration</h3>
<p>To add machine learning predictions (xG, etc.):</p>
<pre><code><h1>1. Add prediction module</h1>
<h1>src/etl/ml_predictions.py</h1>
<p>import joblib</p>
<p>def add_xg_predictions(events_df):
    model = joblib.load('models/xg_model.pkl')
    
    features = events_df[['shot_type', 'distance', 'angle', 'is_rebound']]
    events_df['xg'] = model.predict_proba(features)[:, 1]
    
    return events_df</p>
<h1>2. Call from main ETL</h1>
events_df = add_xg_predictions(events_df)</code></pre>
<h3>Tracking Data (CV)</h3>
<p>For computer vision tracking data:</p>
<pre><code><h1>New tables for position data</h1>
<h1>fact_player_xy_long - player positions over time</h1>
<h1>fact_puck_xy_long - puck positions over time</h1>
<p>def process_tracking_data(video_id):
    # Load CV output
    positions = load_tracking_csv(f'data/tracking/{video_id}_positions.csv')
    
    # Transform to schema
    player_xy = transform_player_positions(positions)
    puck_xy = transform_puck_positions(positions)
    
    # Output
    player_xy.to_csv('data/output/fact_player_xy_long.csv')
    puck_xy.to_csv('data/output/fact_puck_xy_long.csv')</code></pre></p>
<h3>NHL Data Integration</h3>
<p>To incorporate NHL data for comparison:</p>
<pre><code><h1>1. Add NHL API client</h1>
<h1>src/etl/nhl_api.py</h1>
<p>import requests</p>
<p>def fetch_nhl_game(nhl_game_id):
    url = f"https://statsapi.web.nhl.com/api/v1/game/{nhl_game_id}/feed/live"
    response = requests.get(url)
    return response.json()</p>
<h1>2. Map NHL players to internal IDs</h1>
def map_nhl_to_internal(nhl_player_id):
    # Use mapping table
    mapping = pd.read_csv('data/raw/nhl_player_mapping.csv')
    return mapping[mapping['nhl_id'] == nhl_player_id]['player_id'].iloc[0]
<h1>3. Create comparison tables</h1>
<h1>fact_nhl_comparison - side-by-side with NHL averages</h1></code></pre>
<hr>
<h2>Monitoring & Alerting</h2>
<h3>Log Monitoring</h3>
<pre><code><h1>Check for errors in logs</h1>
import glob
<p>def check_recent_errors():
    log_files = glob.glob('logs/*<em>/</em>.log', recursive=True)
    for log_file in log_files:
        with open(log_file) as f:
            for line in f:
                if 'ERROR' in line:
                    print(f"{log_file}: {line}")</code></pre></p>
<h3>Automated Validation</h3>
<pre><code><h1>Run after every ETL</h1>
def post_etl_validation():
    import subprocess
    result = subprocess.run(
        ['python', '-m', 'pytest', 'tests/', '-v', '--tb=short'],
        capture_output=True
    )
    if result.returncode != 0:
        send_alert("ETL validation failed!")
    return result.returncode == 0</code></pre>
<hr>
<h2>Troubleshooting</h2>
<h3>"File not found"</h3>
<pre><code><h1>Check file exists</h1>
ls data/raw/18969/
<h1>Check file format</h1>
file data/raw/18969/events_tracking.xlsx</code></pre>
<h3>"Column not found"</h3>
<pre><code><h1>Check column names in source</h1>
python -c "import pandas as pd; print(pd.read_excel('data/raw/18969/events_tracking.xlsx').columns.tolist())"</code></pre>
<h3>"Data type error"</h3>
<pre><code><h1>Debug data types</h1>
df = pd.read_csv('data/output/fact_events.csv')
print(df.dtypes)
print(df['problematic_column'].unique())</code></pre>
<h3>"Memory error"</h3>
<pre><code><h1>Increase memory limit or process in chunks</h1>
python etl.py --chunk-size 5000</code></pre>
<hr>
<h2>Maintenance Checklist</h2>
<h3>Daily</h3>
<ul>
<li>[ ] Check log files for errors</li>
<li>[ ] Verify new games processed correctly</li>
</ul>
<h3>Weekly</h3>
<ul>
<li>[ ] Run full validation suite</li>
<li>[ ] Check data quality metrics</li>
<li>[ ] Review any flagged issues</li>
</ul>
<h3>Monthly</h3>
<ul>
<li>[ ] Backup database</li>
<li>[ ] Clean old log files</li>
<li>[ ] Review and update documentation</li>
<li>[ ] Check for dependency updates</li>
</ul>
<h3>Quarterly</h3>
<ul>
<li>[ ] Performance review</li>
<li>[ ] Schema optimization</li>
<li>[ ] Review and archive old data</li>
<li>[ ] Plan enhancements</li>
</ul>
<hr>
<h2>Resources</h2>
<ul>
<li>ETL Code: <code>etl.py</code> and <code>src/etl/</code></li>
<li>Tests: <code>tests/</code></li>
<li>Logs: <code>logs/</code></li>
<li>Config: <code>config/</code></li>
<li>Raw Data: <code>data/raw/</code></li>
<li>Output: <code>data/output/</code></li>
</ul>

</body>
</html>